{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing different embedding schemes for information retrieval\n",
    "## Step 1: Load sample text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda/envs/py35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from src.utils import read_txt, split_txt, aiap_qna_quickscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = read_txt('../data/fund_guide_extend.txt')\n",
    "# text = [x.decode(\"utf-8\") for x in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "condition_terms = split_txt(text, qa=False)\n",
    "print(len(condition_terms))\n",
    "# condition_terms[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Load sample questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_queries = pd.read_csv('../data/Consolidated emails.csv', encoding='iso-8859-1')\n",
    "print(len(df_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer collection\n",
    "initial_list=df_queries['Email Queries'].values\n",
    "modified_list=[val for val in initial_list for _ in (0, 1, 2)]\n",
    "df_comparisons = pd.DataFrame({'queries' : modified_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: define similarity function\n",
    "The cosine similarity function returns the cosine similarity given a query string, an encoder, and an array of knowledgebase embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim_results(query_str, encoder, kb_embeddings, **kwargs):\n",
    "    if kwargs:\n",
    "        qn_embedding = encoder(query_str, kwargs.get('tokenize', None))\n",
    "    else:\n",
    "        qn_embedding = encoder(query_str)\n",
    "    results = cosine_similarity(kb_embeddings, qn_embedding)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_embed_fn(vector):\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp 1: Test InferSent model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from InferSent.models import InferSent\n",
    "import torch\n",
    "V = 1\n",
    "MODEL_PATH = '../encoder/infersent%s.pkl' % V\n",
    "params_model = {'bsize': 64, 'word_emb_dim': 300, 'enc_lstm_dim': 2048,\n",
    "                'pool_type': 'max', 'dpout_model': 0.0, 'version': V}\n",
    "infersent = InferSent(params_model)\n",
    "infersent.load_state_dict(torch.load(MODEL_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "W2V_PATH = '../fastText/crawl-300d-2M.vec'\n",
    "infersent.set_w2v_path(W2V_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2646(/2721) words with w2v vectors\n",
      "Vocab size : 2646\n"
     ]
    }
   ],
   "source": [
    "infersent.build_vocab(condition_terms, tokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0(/75) words with w2v vectors\n",
      "New vocab size : 2646 (added 0 words)\n"
     ]
    }
   ],
   "source": [
    "infersent.update_vocab(condition_terms, tokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_results = infersent.encode(condition_terms, tokenize=True)\n",
    "questions = list(df_queries['Email Queries'])\n",
    "question_results = infersent.encode(questions, tokenize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(148, 4096) (73, 4096)\n"
     ]
    }
   ],
   "source": [
    "print(response_results.shape, question_results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample question\n",
    "# question=['how frequent can i get disbursements?']\n",
    "# condition_terms[cosine_sim_results(question, infersent.encode, embeddings, tokenize=True).argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question=['who is RESPONSIBLE FOR DATA CHARGES?']\n",
    "# sortargs=cosine_sim_results(question, infersent.encode, embeddings, tokenize=True).argsort(axis=0)\n",
    "# print(sortargs.shape)\n",
    "# for ii,arg in enumerate(sortargs[::-1,0]):\n",
    "#     print(ii, condition_terms[arg])\n",
    "#     if ii==4:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(cosine_sim_results(question, infersent.encode, embeddings, tokenize=True).argsort(axis=0).shape[-2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with example queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses=[]\n",
    "for index, ii in enumerate(df_queries.iterrows()):\n",
    "    print('QN: ', ii[1]['Email Queries'])\n",
    "    sortargs=np.flip(cosine_sim_results(question_results[index].reshape(1, -1), dummy_embed_fn, response_results).argsort(axis=0))\n",
    "    for ans in range(3):\n",
    "        responses.append(condition_terms[sortargs[ans,0]])\n",
    "        print('ANS: ', condition_terms[sortargs[ans,0]])\n",
    "df_comparisons['infersent']=responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# responses=[]\n",
    "# for ii in df_queries.iterrows():\n",
    "#     print('QN: ', ii[1]['Email Queries'])\n",
    "#     sortargs=np.flip(cosine_sim_results([ii[1]['Email Queries']], infersent.encode, embeddings, tokenize=True).argsort(axis=0))\n",
    "#     for ans in range(3):\n",
    "#         print(sortargs[ans, 0])\n",
    "#         responses.append(condition_terms[sortargs[ans,0]])\n",
    "#         print('ANS: ', condition_terms[sortargs[ans,0]])\n",
    "# #     answer = condition_terms[cosine_sim_results([ii[1]['Email Queries']], infersent.encode, embeddings, tokenize=True).argmax()]\n",
    "# #     print('ANS: ', answer)\n",
    "# #     responses.append(answer)\n",
    "# #     print('\\n')\n",
    "# df_comparisons['infersent']=responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp 2: Google universal sentence encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# !pip install tensorflow-gpu\n",
    "# !pip install tensorflow-hub\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the model to local so it can be used again and again\n",
    "# !mkdir google_use\n",
    "# Download the module, and uncompress it to the destination folder. \n",
    "# !curl -L \"https://tfhub.dev/google/universal-sentence-encoder-large/3?tf-hub-format=compressed\" | tar -zxvC ./google_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0913 07:56:00.905652 140286982317824 deprecation.py:323] From /anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "embed = hub.Module(\"../google_use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_embed(terms):\n",
    "    with tf.Session() as session:\n",
    "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        message_embeddings = session.run(embed(terms))\n",
    "    return message_embeddings\n",
    "\n",
    "# to only load session once.\n",
    "# def embed_useT(module):\n",
    "#     with tf.Graph().as_default():\n",
    "#         sentences = tf.placeholder(tf.string)\n",
    "#         embed = hub.Module(module)\n",
    "#         embeddings = embed(sentences)\n",
    "#         session = tf.train.MonitoredSession()\n",
    "#     return lambda x: session.run(embeddings, {sentences: x})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Dst tensor is not initialized.\n\t [[node checkpoint_initializer_20 (defined at /anaconda/envs/py35/lib/python3.5/site-packages/tensorflow_hub/native_module.py:407) ]]\n\nCaused by op 'checkpoint_initializer_20', defined at:\n  File \"/anaconda/envs/py35/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/anaconda/envs/py35/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/anaconda/envs/py35/lib/python3.5/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/anaconda/envs/py35/lib/python3.5/asyncio/base_events.py\", line 1425, in _run_once\n    handle._run()\n  File \"/anaconda/envs/py35/lib/python3.5/asyncio/events.py\", line 127, in _run\n    self._callback(*self._args)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tornado/ioloop.py\", line 759, in _run_callback\n    ret = callback()\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-23-b7be6759c720>\", line 1, in <module>\n    embed = hub.Module(\"./google_use\")\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow_hub/module.py\", line 170, in __init__\n    tags=self._tags)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow_hub/native_module.py\", line 340, in _create_impl\n    name=name)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow_hub/native_module.py\", line 399, in __init__\n    self._init_state(name)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow_hub/native_module.py\", line 407, in _init_state\n    self._variable_map)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/training/checkpoint_utils.py\", line 190, in init_from_checkpoint\n    _init_from_checkpoint, args=(ckpt_dir_or_file, assignment_map))\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 1516, in merge_call\n    return self._merge_call(merge_fn, args, kwargs)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 1524, in _merge_call\n    return merge_fn(self._distribution_strategy, *args, **kwargs)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/training/checkpoint_utils.py\", line 234, in _init_from_checkpoint\n    _set_variable_or_list_initializer(var, ckpt_file, tensor_name_in_ckpt)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/training/checkpoint_utils.py\", line 358, in _set_variable_or_list_initializer\n    _set_checkpoint_initializer(variable_or_list, ckpt_file, tensor_name, \"\")\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/training/checkpoint_utils.py\", line 312, in _set_checkpoint_initializer\n    ckpt_file, [tensor_name], [slice_spec], [base_type], name=name)[0]\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1572, in restore_v2\n    name=name)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInternalError (see above for traceback): Dst tensor is not initialized.\n\t [[node checkpoint_initializer_20 (defined at /anaconda/envs/py35/lib/python3.5/site-packages/tensorflow_hub/native_module.py:407) ]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Dst tensor is not initialized.\n\t [[{{node checkpoint_initializer_20}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-b603f362abf0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresponse_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition_terms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mall_questions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_queries\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Email Queries'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mquestions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_questions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mquestion_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-ef0e368a0c2b>\u001b[0m in \u001b[0;36muse_embed\u001b[0;34m(terms)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0muse_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mmessage_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmessage_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1346\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Dst tensor is not initialized.\n\t [[node checkpoint_initializer_20 (defined at /anaconda/envs/py35/lib/python3.5/site-packages/tensorflow_hub/native_module.py:407) ]]\n\nCaused by op 'checkpoint_initializer_20', defined at:\n  File \"/anaconda/envs/py35/lib/python3.5/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/anaconda/envs/py35/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 486, in start\n    self.io_loop.start()\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tornado/platform/asyncio.py\", line 127, in start\n    self.asyncio_loop.run_forever()\n  File \"/anaconda/envs/py35/lib/python3.5/asyncio/base_events.py\", line 421, in run_forever\n    self._run_once()\n  File \"/anaconda/envs/py35/lib/python3.5/asyncio/base_events.py\", line 1425, in _run_once\n    handle._run()\n  File \"/anaconda/envs/py35/lib/python3.5/asyncio/events.py\", line 127, in _run\n    self._callback(*self._args)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tornado/ioloop.py\", line 759, in _run_callback\n    ret = callback()\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 536, in <lambda>\n    self.io_loop.add_callback(lambda : self._handle_events(self.socket, 0))\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n    self._handle_recv()\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n    callback(*args, **kwargs)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tornado/stack_context.py\", line 276, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2662, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2785, in _run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2903, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2963, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-23-b7be6759c720>\", line 1, in <module>\n    embed = hub.Module(\"./google_use\")\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow_hub/module.py\", line 170, in __init__\n    tags=self._tags)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow_hub/native_module.py\", line 340, in _create_impl\n    name=name)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow_hub/native_module.py\", line 399, in __init__\n    self._init_state(name)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow_hub/native_module.py\", line 407, in _init_state\n    self._variable_map)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/training/checkpoint_utils.py\", line 190, in init_from_checkpoint\n    _init_from_checkpoint, args=(ckpt_dir_or_file, assignment_map))\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 1516, in merge_call\n    return self._merge_call(merge_fn, args, kwargs)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/distribute/distribute_lib.py\", line 1524, in _merge_call\n    return merge_fn(self._distribution_strategy, *args, **kwargs)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/training/checkpoint_utils.py\", line 234, in _init_from_checkpoint\n    _set_variable_or_list_initializer(var, ckpt_file, tensor_name_in_ckpt)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/training/checkpoint_utils.py\", line 358, in _set_variable_or_list_initializer\n    _set_checkpoint_initializer(variable_or_list, ckpt_file, tensor_name, \"\")\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/training/checkpoint_utils.py\", line 312, in _set_checkpoint_initializer\n    ckpt_file, [tensor_name], [slice_spec], [base_type], name=name)[0]\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 1572, in restore_v2\n    name=name)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 788, in _apply_op_helper\n    op_def=op_def)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/util/deprecation.py\", line 507, in new_func\n    return func(*args, **kwargs)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 3300, in create_op\n    op_def=op_def)\n  File \"/anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1801, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nInternalError (see above for traceback): Dst tensor is not initialized.\n\t [[node checkpoint_initializer_20 (defined at /anaconda/envs/py35/lib/python3.5/site-packages/tensorflow_hub/native_module.py:407) ]]\n"
     ]
    }
   ],
   "source": [
    "response_results = use_embed(condition_terms)\n",
    "all_questions = df_queries['Email Queries']\n",
    "questions=list(all_questions)\n",
    "question_results = use_embed(questions)\n",
    "print(response_results.shape, question_results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# responses=[]\n",
    "# for ii in df_queries.iterrows():\n",
    "#     print('QN: ', ii[1]['Email Queries'])\n",
    "#     answer = condition_terms[cosine_sim_results([ii[1]['Email Queries']], use_embed, message_embeddings).argmax()]\n",
    "#     print('ANS: ', answer)\n",
    "#     responses.append(answer)\n",
    "#     print('\\n')\n",
    "# df_comparisons['use']=responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "responses=[]\n",
    "for index, ii in enumerate(df_queries.iterrows()):\n",
    "    print('QN: ', ii[1]['Email Queries'])\n",
    "    sortargs=np.flip(cosine_sim_results(question_results[index].reshape(1, -1), dummy_embed_fn, response_results).argsort(axis=0))\n",
    "    for ans in range(3):\n",
    "        responses.append(condition_terms[sortargs[ans,0]])\n",
    "        print('ANS: ', condition_terms[sortargs[ans,0]])\n",
    "df_comparisons['use']=responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# responses=[]\n",
    "# for ii in df_queries.iterrows():\n",
    "#     print('QN: ', ii[1]['Email Queries'])\n",
    "#     sortargs=np.flip(cosine_sim_results([ii[1]['Email Queries']], use_embed, message_embeddings).argsort(axis=0))\n",
    "#     for ans in range(3):\n",
    "#         responses.append(condition_terms[sortargs[ans,0]])\n",
    "#         print('ANS: ', condition_terms[sortargs[ans,0]])\n",
    "# df_comparisons['use']=responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exp 3: Test the new QnA USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentencepiece\n",
    "!pip install tf-sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import tf_sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    # Set up graph.\n",
    "    g = tf.Graph()\n",
    "    with g.as_default():\n",
    "        embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/1\")\n",
    "        # put placeholders\n",
    "        question = tf.placeholder(dtype=tf.string, shape=[None])  # question\n",
    "        response = tf.placeholder(dtype=tf.string, shape=[None])  # response\n",
    "        response_context = tf.placeholder(dtype=tf.string, shape=[None])  # response context\n",
    "        \n",
    "        question_embeddings = embed(\n",
    "        dict(input=question),\n",
    "        signature=\"question_encoder\", as_dict=True)\n",
    "\n",
    "        response_embeddings = embed(\n",
    "        dict(input=response,\n",
    "             context=response_context),\n",
    "        signature=\"response_encoder\", as_dict=True)\n",
    "\n",
    "        init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    g.finalize()\n",
    "\n",
    "    # Initialize session.\n",
    "    session = tf.Session(graph=g, config=tf.ConfigProto(log_device_placement=True))\n",
    "    session.run(init_op)\n",
    "    return session, question_embeddings, response_embeddings, question, response, response_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define inputs\n",
    "responses = condition_terms\n",
    "response_contexts = responses # no need to provide context\n",
    "all_questions = df_queries['Email Queries']\n",
    "questions=list(all_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session, question_embeddings, response_embeddings, question, response, response_context = init_model(questions, responses, response_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute embeddings.\n",
    "response_results = session.run(response_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_results = session.run(question_embeddings)\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_results['outputs'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# responses = []\n",
    "# for index, ii in enumerate(df_queries.iterrows()):\n",
    "#     print('QN: ', ii[1]['Email Queries'])\n",
    "#     answer=condition_terms[cosine_sim_results(question_results['outputs'][index].reshape(1, -1), dummy_embed_fn, response_results['outputs']).argmax()]\n",
    "#     print('ANS: ', answer)\n",
    "#     responses.append(answer)\n",
    "#     print('\\n')\n",
    "# df_comparisons['use_qa']=responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses=[]\n",
    "for index, ii in enumerate(df_queries.iterrows()):\n",
    "    print('QN: ', ii[1]['Email Queries'])\n",
    "    sortargs=np.flip(cosine_sim_results(question_results['outputs'][index].reshape(1, -1), dummy_embed_fn, response_results['outputs']).argsort(axis=0))\n",
    "    for ans in range(3):\n",
    "        responses.append(condition_terms[sortargs[ans,0]])\n",
    "        print('ANS: ', condition_terms[sortargs[ans,0]])\n",
    "df_comparisons['use_qa']=responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comparisons.to_csv('../predictions_3_extend_batch2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test AIAP qna\n",
    "7/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0920 06:57:38.990347 139906543216384 deprecation.py:323] From /anaconda/envs/py35/lib/python3.5/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0920 06:57:54.408357 139906543216384 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0920 06:58:00.903040 139906543216384 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "from src.model import QnaEncoderModel\n",
    "model=QnaEncoderModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are looking for candidates who possess a keen interest in the area of machine learning and data science. We believe that candidates can come from any area of specialisation, and our requirements are as follow: i)   Singaporean with a polytechnic diploma or university degree, ii) Proficient in Python or R and iii) Is able to implement Machine Learning Algorithms or have a background in Mathematics / Statistics / Computer Science.  Beyond that, demonstrated statistical fundamentals and programming ability will be helpful for the technical tests, but a keen learning attitude will be the most important to carry you through the programme.   Q1. WHAT SORT OF CANDIDATES ARE YOU LOOKING FOR?\n"
     ]
    }
   ],
   "source": [
    "aiap_qa, aiap_questions = split_txt(read_txt('../data/aiap.txt'), qa=True)\n",
    "# aiap_qa = split_txt(read_txt('./data/aiap.txt'))\n",
    "print(aiap_qa[0], aiap_questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_array = model.predict(aiap_qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     return aiap_qa[(lambda x: cosine_similarity(answer_array, model.predict(x, type='query')))([question]).argmax()]\n",
    "# aiap_qa[np.flip(cosine_sim_results(question_results['outputs'][index].reshape(1, -1), dummy_embed_fn, response_results['outputs']).argsort(axis=0))[:2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "aiap_qna_quickscore(aiap_questions, answer_array, aiap_qa, model, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1. WHAT SORT OF CANDIDATES ARE YOU LOOKING FOR? (['Candidates can expect to be equipped in some or all of the following skills: data modelling/tuning, data engineering, data product-related software engineering, cloud applications. It ranges between individuals, but candidates can be adequately prepared in fields of data science, engineering and consultancy '], [3, 0, 6, 4, 9, 5, 8, 1, 2, 7]) \n",
      "\n",
      "Q2. WHAT WILL BE COVERED IN THE PROGRAMME? (['We will begin with some structured coursework involving programming, modelling and deployment. Following which, apprentices will be assigned to 100E projects to hone their skills in-depth, specific to each project. Generally, you will be doing the following: Deploy Machine Learning applications in line with industry best practices Select, train and tune Machine Learning algorithms Design and build Data Pipelines and Infrastructures for AI Use Cases '], [1, 7, 5, 0, 3, 6, 8, 9, 4, 2]) \n",
      "\n",
      "Q3. DO I HAVE TO PAY? (['If you drop out of the course, you will have to reimburse AISG the training stipends paid out to you and the fees for the programme. '], [7, 5, 6, 2, 9, 8, 4, 0, 1, 3]) \n",
      "\n",
      "Q4. WHAT IS THE OUTCOME OF THIS PROGRAMME? (['YES. This is a FULL TIME programme where you will undergo training at an intensive pace over 9 months.  Due to our sponsorship requirements, you are not allowed to take no-pay leave to attend the programme. '], [6, 5, 7, 0, 1, 3, 9, 2, 8, 4]) \n",
      "\n",
      "Q5. WILL I GET A JOB AFTER THE PROGRAMME? (['Yes, we run 3 iterations of the programme every year, about 4 months apart. You may apply again if you are unsuccessful or unavailable for the current batch. '], [5, 7, 6, 4, 2, 3, 9, 0, 1, 8]) \n",
      "\n",
      "Q6. WILL THERE BE A NEXT ROUND OF APPLICATION? (['Yes, we run 3 iterations of the programme every year, about 4 months apart. You may apply again if you are unsuccessful or unavailable for the current batch. '], [5, 8, 3, 6, 1, 0, 9, 7, 4, 2]) \n",
      "\n",
      "Q7. DO I HAVE TO GIVE UP MY CURRENT JOB TO JOIN THE  PROGRAMME? (['YES. This is a FULL TIME programme where you will undergo training at an intensive pace over 9 months.  Due to our sponsorship requirements, you are not allowed to take no-pay leave to attend the programme. '], [6, 5, 7, 2, 4, 9, 0, 3, 1, 8]) \n",
      "\n",
      "Q8. WHAT IF I DECIDED TO DROP OUT OF THE PROGRAMME? (['If you drop out of the course, you will have to reimburse AISG the training stipends paid out to you and the fees for the programme. '], [7, 6, 5, 1, 9, 3, 0, 4, 2, 8]) \n",
      "\n",
      "Q9. WILL I GET SOME SORT OF CERTIFICATE TO SHOW THAT I PARTICIPATED IN THE AIAP? (['Apprentices who have successfully completed the AIAP will receive a certificate from AISG. '], [8, 6, 7, 5, 9, 2, 4, 3, 0, 1]) \n",
      "\n",
      "Q10. WHAT ARE THE AI JOBS AND ROLES I CAN CONSIDER AFTER COMPLETING THE TRAINING PROGRAMME? (['We will begin with some structured coursework involving programming, modelling and deployment. Following which, apprentices will be assigned to 100E projects to hone their skills in-depth, specific to each project. Generally, you will be doing the following: Deploy Machine Learning applications in line with industry best practices Select, train and tune Machine Learning algorithms Design and build Data Pipelines and Infrastructures for AI Use Cases '], [1, 6, 7, 0, 9, 4, 8, 3, 2, 5]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for qn in aiap_context:\n",
    "    print(qn, aiap_qna(qn, answer_arraym, aiap_qa, model, 1), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USE model\n",
    "2/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0917 05:40:46.944837 140157478967040 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1. WHAT SORT OF CANDIDATES ARE YOU LOOKING FOR? Candidates can expect to be equipped in some or all of the following skills: data modelling/tuning, data engineering, data product-related software engineering, cloud applications. It ranges between individuals, but candidates can be adequately prepared in fields of data science, engineering and consultancy  \n",
      "\n",
      "Q2. WHAT WILL BE COVERED IN THE PROGRAMME? YES. This is a FULL TIME programme where you will undergo training at an intensive pace over 9 months.  Due to our sponsorship requirements, you are not allowed to take no-pay leave to attend the programme.  \n",
      "\n",
      "Q3. DO I HAVE TO PAY? If you drop out of the course, you will have to reimburse AISG the training stipends paid out to you and the fees for the programme.  \n",
      "\n",
      "Q4. WHAT IS THE OUTCOME OF THIS PROGRAMME? YES. This is a FULL TIME programme where you will undergo training at an intensive pace over 9 months.  Due to our sponsorship requirements, you are not allowed to take no-pay leave to attend the programme.  \n",
      "\n",
      "Q5. WILL I GET A JOB AFTER THE PROGRAMME? YES. This is a FULL TIME programme where you will undergo training at an intensive pace over 9 months.  Due to our sponsorship requirements, you are not allowed to take no-pay leave to attend the programme.  \n",
      "\n",
      "Q6. WILL THERE BE A NEXT ROUND OF APPLICATION? We are looking for candidates who possess a keen interest in the area of machine learning and data science. We believe that candidates can come from any area of specialisation, and our requirements are as follow: i)   Singaporean with a polytechnic diploma or university degree, ii) Proficient in Python or R and iii) Is able to implement Machine Learning Algorithms or have a background in Mathematics / Statistics / Computer Science.  Beyond that, demonstrated statistical fundamentals and programming ability will be helpful for the technical tests, but a keen learning attitude will be the most important to carry you through the programme.   \n",
      "\n",
      "Q7. DO I HAVE TO GIVE UP MY CURRENT JOB TO JOIN THE  PROGRAMME? YES. This is a FULL TIME programme where you will undergo training at an intensive pace over 9 months.  Due to our sponsorship requirements, you are not allowed to take no-pay leave to attend the programme.  \n",
      "\n",
      "Q8. WHAT IF I DECIDED TO DROP OUT OF THE PROGRAMME? YES. This is a FULL TIME programme where you will undergo training at an intensive pace over 9 months.  Due to our sponsorship requirements, you are not allowed to take no-pay leave to attend the programme.  \n",
      "\n",
      "Q9. WILL I GET SOME SORT OF CERTIFICATE TO SHOW THAT I PARTICIPATED IN THE AIAP? Apprentices who have successfully completed the AIAP will receive a certificate from AISG.  \n",
      "\n",
      "Q10. WHAT ARE THE AI JOBS AND ROLES I CAN CONSIDER AFTER COMPLETING THE TRAINING PROGRAMME? We are looking for candidates who possess a keen interest in the area of machine learning and data science. We believe that candidates can come from any area of specialisation, and our requirements are as follow: i)   Singaporean with a polytechnic diploma or university degree, ii) Proficient in Python or R and iii) Is able to implement Machine Learning Algorithms or have a background in Mathematics / Statistics / Computer Science.  Beyond that, demonstrated statistical fundamentals and programming ability will be helpful for the technical tests, but a keen learning attitude will be the most important to carry you through the programme.   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.model import USEModel\n",
    "model=USEModel()\n",
    "aiap_qa, aiap_context = split_txt(read_txt('../data/aiap.txt'), qa=True)\n",
    "answer_array = model.predict(aiap_qa)\n",
    "def aiap_qna(question):\n",
    "    return aiap_qa[(lambda x: cosine_similarity(answer_array, model.predict(x)))([question]).argmax()]\n",
    "for qn in aiap_context:\n",
    "    print(qn, aiap_qna(qn), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infersent\n",
    "build vocab on context only: 1/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 65(/67) words with w2v vectors\n",
      "Vocab size : 65\n",
      "Q1. WHAT SORT OF CANDIDATES ARE YOU LOOKING FOR? No payment is required. All apprentices will receive full sponsorship from AI Singapore and Infocomm and Media Development Authority.  The apprentices will also receive a stipend of between SGD$3,500 to $5,500/month depending on your working experience.  \n",
      "\n",
      "Q2. WHAT WILL BE COVERED IN THE PROGRAMME? No payment is required. All apprentices will receive full sponsorship from AI Singapore and Infocomm and Media Development Authority.  The apprentices will also receive a stipend of between SGD$3,500 to $5,500/month depending on your working experience.  \n",
      "\n",
      "Q3. DO I HAVE TO PAY? No payment is required. All apprentices will receive full sponsorship from AI Singapore and Infocomm and Media Development Authority.  The apprentices will also receive a stipend of between SGD$3,500 to $5,500/month depending on your working experience.  \n",
      "\n",
      "Q4. WHAT IS THE OUTCOME OF THIS PROGRAMME? No payment is required. All apprentices will receive full sponsorship from AI Singapore and Infocomm and Media Development Authority.  The apprentices will also receive a stipend of between SGD$3,500 to $5,500/month depending on your working experience.  \n",
      "\n",
      "Q5. WILL I GET A JOB AFTER THE PROGRAMME? No payment is required. All apprentices will receive full sponsorship from AI Singapore and Infocomm and Media Development Authority.  The apprentices will also receive a stipend of between SGD$3,500 to $5,500/month depending on your working experience.  \n",
      "\n",
      "Q6. WILL THERE BE A NEXT ROUND OF APPLICATION? No payment is required. All apprentices will receive full sponsorship from AI Singapore and Infocomm and Media Development Authority.  The apprentices will also receive a stipend of between SGD$3,500 to $5,500/month depending on your working experience.  \n",
      "\n",
      "Q7. DO I HAVE TO GIVE UP MY CURRENT JOB TO JOIN THE  PROGRAMME? No payment is required. All apprentices will receive full sponsorship from AI Singapore and Infocomm and Media Development Authority.  The apprentices will also receive a stipend of between SGD$3,500 to $5,500/month depending on your working experience.  \n",
      "\n",
      "Q8. WHAT IF I DECIDED TO DROP OUT OF THE PROGRAMME? No payment is required. All apprentices will receive full sponsorship from AI Singapore and Infocomm and Media Development Authority.  The apprentices will also receive a stipend of between SGD$3,500 to $5,500/month depending on your working experience.  \n",
      "\n",
      "Q9. WILL I GET SOME SORT OF CERTIFICATE TO SHOW THAT I PARTICIPATED IN THE AIAP? No payment is required. All apprentices will receive full sponsorship from AI Singapore and Infocomm and Media Development Authority.  The apprentices will also receive a stipend of between SGD$3,500 to $5,500/month depending on your working experience.  \n",
      "\n",
      "Q10. WHAT ARE THE AI JOBS AND ROLES I CAN CONSIDER AFTER COMPLETING THE TRAINING PROGRAMME? No payment is required. All apprentices will receive full sponsorship from AI Singapore and Infocomm and Media Development Authority.  The apprentices will also receive a stipend of between SGD$3,500 to $5,500/month depending on your working experience.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.model import InferSent\n",
    "model=InferSent()\n",
    "aiap_qa, aiap_context = split_txt(read_txt('../data/aiap.txt'), qa=True)\n",
    "model.build_vocab(aiap_context)\n",
    "answer_array = model.predict(aiap_qa)\n",
    "def aiap_qna(question):\n",
    "    return aiap_qa[(lambda x: cosine_similarity(answer_array, model.predict(x)))([question]).argmax()]\n",
    "for qn in aiap_context:\n",
    "    print(qn, aiap_qna(qn), '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

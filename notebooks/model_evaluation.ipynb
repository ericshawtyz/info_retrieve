{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model performance against insurance test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda/envs/py35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from src.utils import question_cleaner, display_qn_and_ans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import logging\n",
    "logging.basicConfig(filename='evaluation.log',level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load QA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath=Path('../data')\n",
    "df_query = pd.read_csv(datapath/'insuranceQA/V2/InsuranceQA.question.anslabel.raw.100.pool.solr.test.encoded', delimiter='\\t', header=None)\n",
    "df_doc = pd.read_csv(datapath/'insuranceQA/V2/InsuranceQA.label2answer.raw.encoded', delimiter='\\t', header=None)\n",
    "df_ind2word = pd.read_csv(datapath/'insuranceQA/V2/vocabulary', sep='\\t', header=None, quotechar='', quoting=3, keep_default_na=False)\n",
    "dict_ind2word = pd.Series(df_ind2word[1].values,index=df_ind2word[0].values).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract only questions that have answers\n",
    "The dataset has this weird thing where the questions that have no correct answers have random answers in the answer column that does not match the question.\n",
    "Also set the index for df_doc for easy reference with .loc later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:2000, removed:677, remainder:1323\n"
     ]
    }
   ],
   "source": [
    "df_query=question_cleaner(df_query)\n",
    "df_doc=df_doc.set_index(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert from tokens to full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>medicare-insurance</td>\n",
       "      <td>idx_2363 idx_467 idx_8080 idx_31 idx_9966 idx_...</td>\n",
       "      <td>9128</td>\n",
       "      <td>9128 13322 21601 21471 6442 5412 24861 23536 2...</td>\n",
       "      <td>Will Medicare Pay For Smoking Cessation?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0                                                  1  \\\n",
       "4  medicare-insurance  idx_2363 idx_467 idx_8080 idx_31 idx_9966 idx_...   \n",
       "\n",
       "      2                                                  3  \\\n",
       "4  9128  9128 13322 21601 21471 6442 5412 24861 23536 2...   \n",
       "\n",
       "                                       text  \n",
       "4  Will Medicare Pay For Smoking Cessation?  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>idx_1 idx_2 idx_3 idx_4 idx_5 idx_6 idx_7 idx_...</td>\n",
       "      <td>Coverage follows the car. Example 1: If you we...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   1  \\\n",
       "0                                                      \n",
       "1  idx_1 idx_2 idx_3 idx_4 idx_5 idx_6 idx_7 idx_...   \n",
       "\n",
       "                                                text  \n",
       "0                                                     \n",
       "1  Coverage follows the car. Example 1: If you we...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def wordifier(tokes):\n",
    "    return ' '.join([dict_ind2word[ind] for ind in tokes.strip().split(' ')])\n",
    "df_doc['text']=df_doc.apply(lambda x: wordifier(x[1]), axis=1)\n",
    "df_query['text']=df_query.apply(lambda x: wordifier(x[1]), axis=1)\n",
    "display(df_query.head(1))\n",
    "display(df_doc.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question is: Will Medicare Pay For Smoking Cessation?\n",
      "Answer index:  [9128]\n",
      "Answers:  ['Medicare will not pay for smoking cessation products such as nicotine substitutes (Nicorette gum, nicotine patch, etc), and Medicare will not pay for pills that reduce the craving to smoke. But Medicare will pay for up to 8 face-to-face smoking cessation counseling sessions with a qualified Medicare doctor during a 12 month period.']\n"
     ]
    }
   ],
   "source": [
    "display_qn_and_ans(df_query, df_doc, index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranker(model, question_vectors, df_query, df_doc):\n",
    "    predictions=[]\n",
    "    gts=[]\n",
    "    for ii, question_vector in enumerate(question_vectors):\n",
    "        kb=[int(xx) for xx in (df_query[3].iloc[ii]).split(' ')]\n",
    "        gt = [int(xx) for xx in (df_query[2].iloc[ii]).split(' ')]\n",
    "        doc_vectors = model.predict(df_doc.loc[kb]['text'].tolist())\n",
    "        cossim = cosine_similarity(doc_vectors, question_vector.reshape(1, -1))\n",
    "        sortargs=np.flip(cossim.argsort(axis=0))\n",
    "        returnedans = [kb[jj[0]] for jj in sortargs]\n",
    "        predictions.append(returnedans)\n",
    "        gts.append(gt)\n",
    "    return predictions, gts\n",
    "        \n",
    "def scorer(predictions, gts, k=3):\n",
    "    'returns score@k'\n",
    "    score=0\n",
    "    total=0\n",
    "    for gt, prediction in zip(gts, predictions):\n",
    "        if bool(set(gt) & set(prediction[:k])):\n",
    "            score+=1\n",
    "        total+=1\n",
    "    return score/total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USE QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘google_use_qa’: File exists\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "assets/\n",
      "saved_model.pb\n",
      "tfhub_module.pb\n",
      "variables/\n",
      "variables/variables.index\n",
      "variables/variables.data-00000-of-00001\n",
      "100  317M  100  317M    0     0  82.3M      0  0:00:03  0:00:03 --:--:-- 96.8M\n"
     ]
    }
   ],
   "source": [
    "# !mkdir google_use_qa\n",
    "# !curl -L \"https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/1?tf-hub-format=compressed\" | tar -zxvC ./google_use_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/anaconda/envs/py35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model initiated!\n",
      "questions vectorized!\n",
      "Score @1: 0.3870\n",
      "Score @2: 0.5193\n",
      "Score @3: 0.5896\n",
      "Score @4: 0.6478\n",
      "Score @5: 0.6977\n",
      "CPU times: user 10min 31s, sys: 4min 58s, total: 15min 29s\n",
      "Wall time: 17min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from src.model import GoldenRetriever\n",
    "model = GoldenRetriever()\n",
    "question_vectors = model.predict(df_query['text'].tolist(), type='query')\n",
    "print('questions vectorized!')\n",
    "predictions, gts = ranker(model, question_vectors, df_query, df_doc)\n",
    "for k in range(5):\n",
    "    print('Score @{}: {:.4f}'.format(k+1, scorer(predictions, gts, k+1)))\n",
    "model.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## USE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score @1: 0.2509\n",
      "Score @2: 0.3462\n",
      "Score @3: 0.4271\n",
      "Score @4: 0.4807\n",
      "Score @5: 0.5344\n",
      "CPU times: user 7min 28s, sys: 5min 4s, total: 12min 32s\n",
      "Wall time: 3min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from src.model import USEModel\n",
    "model = USEModel()\n",
    "question_vectors = model.predict(df_query['text'].tolist())\n",
    "predictions, gts = ranker(model, question_vectors, df_query, df_doc)\n",
    "for k in range(5):\n",
    "    print('Score @{}: {:.4f}'.format(k+1, scorer(predictions, gts, k+1)))\n",
    "model.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infersent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1408(/1417) words with w2v vectors\n",
      "Vocab size : 1408\n",
      "Found 25628(/31486) words with w2v vectors\n",
      "New vocab size : 27036 (added 25628 words)\n",
      "Score @1: 0.0831\n",
      "Score @2: 0.1338\n",
      "Score @3: 0.1814\n",
      "Score @4: 0.2260\n",
      "Score @5: 0.2683\n",
      "CPU times: user 17min 56s, sys: 8min 13s, total: 26min 10s\n",
      "Wall time: 14min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# increased batch size from 32 -> 256, moved to gpu.\n",
    "from src.model import InferSent\n",
    "model = InferSent()\n",
    "model.infersent.cuda()\n",
    "model.build_vocab(df_query['text'].tolist())\n",
    "model.update_vocab(df_doc['text'].tolist())\n",
    "question_vectors = model.predict(df_query['text'].tolist())\n",
    "predictions, gts = ranker(model, question_vectors, df_query, df_doc)\n",
    "for k in range(5):\n",
    "    print('Score @{}: {:.4f}'.format(k+1, scorer(predictions, gts, k+1)))\n",
    "    logging.info('Score @{}: {:.4f}'.format(k+1, scorer(predictions, gts, k+1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on a tfidf baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class bow_baseline():\n",
    "    def __init__(self):\n",
    "        self.vectorizer=TfidfVectorizer()\n",
    "\n",
    "    def fit(self, text):\n",
    "        self.vectorizer.fit(text)\n",
    "        \n",
    "    def predict(self, text):\n",
    "        return self.vectorizer.transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score @1: 0.2457\n",
      "Score @2: 0.3492\n",
      "Score @3: 0.4127\n",
      "Score @4: 0.4611\n",
      "Score @5: 0.4989\n",
      "CPU times: user 20.9 s, sys: 0 ns, total: 20.9 s\n",
      "Wall time: 20.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = bow_baseline()\n",
    "model.fit(df_query['text'].tolist())\n",
    "model.fit(df_doc['text'].tolist())\n",
    "question_vectors = model.predict(df_query['text'].tolist())\n",
    "predictions, gts = ranker(model, question_vectors, df_query, df_doc)\n",
    "for k in range(5):\n",
    "    print('Score @{}: {:.4f}'.format(k+1, scorer(predictions, gts, k+1)))\n",
    "    logging.info('Score @{}: {:.4f}'.format(k+1, scorer(predictions, gts, k+1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "## GoldenRetriever Demo\n",
    "\n",
    "#### High level example\n",
    "When given a question, GoldenRetriever will predict the most appropriate response to the question.  \n",
    "\n",
    "    \n",
    "This is done in two steps:\n",
    "1. Golden Retriever takes in a knowledge base in the form of a list of string sentences. \n",
    "2. It then gives the top `top_k` responses in its knowledge base. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vinitrinh/.conda/envs/hotdoc/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vinitrinh/.conda/envs/hotdoc/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vinitrinh/.conda/envs/hotdoc/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:331: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vinitrinh/.conda/envs/hotdoc/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:331: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vinitrinh/.conda/envs/hotdoc/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/vinitrinh/.conda/envs/hotdoc/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model initiated!\n",
      "knowledge base lock and loaded!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['I hate Mondays.'], array([[0.20694011]], dtype=float32))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gain access to src and data folders\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.model import GoldenRetriever\n",
    "import tf_sentencepiece\n",
    "\n",
    "# 1. Init \n",
    "gr = GoldenRetriever()\n",
    "\n",
    "# 2. Load knowledge base\n",
    "gr.load_kb(text_list=['I love my chew toy!', 'I hate Mondays.'])\n",
    "\n",
    "# 3. Given a question, it will predict top response\n",
    "gr.make_query('what do you not love?', top_k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "    \n",
    "#### 1. Initializing Golden Retriever\n",
    "The model is specified within the Graph API:\n",
    "<br>\n",
    "1. GoldenRetriever needs to initialize two encoders; one for questions and one for responses.\n",
    "<br>\n",
    "2. The cosine similarities of the encoded responses and question are then calculated. The possible responses are ranked according to this similarity. \n",
    "<img src='img/Golden Retriever 2 Embeddings.png'>\n",
    "\n",
    "The two encoders are initalized in Tensorflow's Graph API below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model initiated!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import tf_sentencepiece\n",
    "from src.metric_learning import triplet_loss, contrastive_loss\n",
    "from tensorflow.train import Saver\n",
    "from src.utils import split_txt, read_txt, clean_txt, read_kb_csv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Set up graph.\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "\n",
    "    # load USE as a 'module', which is tf_hub's interfacable transfer learning component\n",
    "    embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/1\", trainable=True)\n",
    "    \n",
    "    # put variable placeholders\n",
    "    question = tf.placeholder(dtype=tf.string, shape=[None]) \n",
    "    \n",
    "    response = tf.placeholder(dtype=tf.string, shape=[None]) \n",
    "    response_context = tf.placeholder(dtype=tf.string, shape=[None])\n",
    "\n",
    "    # init the embeddings\n",
    "    question_embeddings = embed(dict(input=question),\n",
    "                                signature=\"question_encoder\", as_dict=True)\n",
    "    response_embeddings = embed(dict(input=response,\n",
    "                                     context=response_context),\n",
    "                                signature=\"response_encoder\", as_dict=True)\n",
    "    \n",
    "    # init session \n",
    "    init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    \n",
    "# Initialize session\n",
    "session = tf.Session(graph=g, config=tf.ConfigProto(log_device_placement=False))\n",
    "session.run(init_op)\n",
    "print('model initiated!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "#### 2. Load knowledge bases\n",
    "The definition of 'loaded' is when the model has saved the knowledge base and is able to answer a relevant question.  \n",
    "This is done by:\n",
    "1. inputting the list of string sentences, `list_of_possible_responses`, and\n",
    "2. passing it through the responses embedding and save the resulting array of encoded responses, `array_of_encoded_responses`. The shape of this array is (Number of responses x embedding size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded responses shape: (2, 512)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.07049897,  0.06177055,  0.02428828, ..., -0.0170604 ,\n",
       "         0.04470512, -0.0412127 ],\n",
       "       [-0.04005127,  0.04781166,  0.01393019, ..., -0.02000583,\n",
       "         0.04993735,  0.02181664]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_possible_responses = ['I love my chew toy!', \n",
    "                              'I hate Mondays.']\n",
    "\n",
    "array_of_encoded_responses = session.run(response_embeddings, \n",
    "                                         feed_dict={response:list_of_possible_responses,\n",
    "                                                    response_context:list_of_possible_responses\n",
    "                                                   })['outputs']\n",
    "\n",
    "print(\"encoded responses shape: {}\".format(array_of_encoded_responses.shape))\n",
    "array_of_encoded_responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "    \n",
    "#### 3. Rank the sentences and return the best answer\n",
    "The question, `question_string`, is passed through the encoder and the resulting vector representation of the question, `encoded_ques`, is kept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded question shape: (1, 512)\n"
     ]
    }
   ],
   "source": [
    "question_string = ['what do you not love?']\n",
    "\n",
    "encoded_ques= session.run(question_embeddings, \n",
    "                          feed_dict={question:question_string,\n",
    "                                    })['outputs']\n",
    "print(\"encoded question shape: {}\".format(encoded_ques.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "Rank the appropriateness of saved responses to the question by cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity score:\n",
      "[[0.16681367]\n",
      " [0.20694011]]\n",
      "\n",
      "Best answer: I hate Mondays.\n"
     ]
    }
   ],
   "source": [
    "similarity_score=cosine_similarity(array_of_encoded_responses, encoded_ques)\n",
    "print(\"Similarity score:\")\n",
    "print(similarity_score)\n",
    "\n",
    "print('')\n",
    "\n",
    "print(\"Best answer: {}\".format(list_of_possible_responses[similarity_score.argmax()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "### 4. Loading databases\n",
    "The data, just before it is fed into the model, needs to be in Question and Answer pairs.   \n",
    "This can be done either by loading data that (a) is already saved in such a format or by splitting long texts into their Question Answer pairs. \n",
    "    \n",
    "#### 4a. Loading PDPA QnA\n",
    "Scraped PDPA data is saved conveniently in Question Answer pairs. Simply cleaning is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/pdpa.csv')\n",
    "df['kb'] = df['meta']+df['answer']\n",
    "\n",
    "def clean_1_txt(text):\n",
    "    \"\"\"Strips formatting\"\"\"\n",
    "    text=text.replace('\\n', '. ') # not sure how newlines are tokenized\n",
    "    text=text.replace('.. ', '. ').rstrip() # remove artifact\n",
    "    return text\n",
    "\n",
    "answers_, ques_ = df['kb'].apply(clean_1_txt), df['question'].apply(clean_1_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "#### 4b. Reading AIAP QnA\n",
    "The AIAP QnA text, however, requires splitting the text into question and answer pairs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 questions\n",
      "\n",
      "Q1. WHAT SORT OF CANDIDATES ARE YOU LOOKING FOR?\n",
      "\n",
      "We are looking for candidates who possess a keen interest in the area of machine learning and data science. We believe that candidates can come from any area of specialisation, and our requirements are as follow:\n",
      "i)   Singaporean with a polytechnic diploma or university degree,\n",
      "ii) Proficient in Python or R and iii) Is able to implement Machine Learning Algorithms or have a background in Mathematics / Statistics / Computer Science. \n",
      "Beyond that, demonstrated statistical fundamentals and programming ability will be helpful for the technical tests, but a keen learning attitude will be the most important to carry you through the programme. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def split_txt(text, qa=False):\n",
    "    \"\"\"\n",
    "    Splits a text document into clauses based on whitespaces. \n",
    "    Additionally, reads a faq document by assuming that the first line is a question \n",
    "    between each whitespaced group\n",
    "    \"\"\"\n",
    "    \n",
    "    # condition_terms save our long strings\n",
    "    # stringg is a placeholder for long stirngs\n",
    "    condition_terms = []\n",
    "    stringg=''\n",
    "    \n",
    "    # the loop goes through the text and save the long string \n",
    "    # when it encounters \\n and whitespace ''\n",
    "    for tex in text:\n",
    "        if (tex=='\\n'):\n",
    "            if (stringg != ''):\n",
    "                condition_terms.append(stringg)\n",
    "                stringg=''\n",
    "            else: pass\n",
    "        else: stringg+=tex\n",
    "          \n",
    "    # now that we have the list of condition_terms\n",
    "    # we may need to split the strings into Questions and Answers\n",
    "    # function takes the first sentence the question and the rest as questions\n",
    "    if qa:\n",
    "        condition_context = [x.split('\\n')[0] for x in condition_terms]\n",
    "        condition_terms = ['\\n'.join(x.split('\\n')[1:]) for x in condition_terms]\n",
    "        return condition_terms, condition_context\n",
    "    else: return condition_terms\n",
    "\n",
    "def read_txt(path):\n",
    "    \"\"\"Used with split_txt() to read and split kb into clauses\"\"\"\n",
    "    with open(path, 'r', encoding=\"utf-8\") as f:\n",
    "        text = f.readlines()\n",
    "    return text\n",
    "    \n",
    "text, questions = split_txt(read_txt('../data/aiap.txt'), qa=True)\n",
    "\n",
    "print(\"{} questions\".format(len(questions)))\n",
    "print('')\n",
    "print(questions[0])\n",
    "print('')\n",
    "print(text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "### 5. Finetune and evaluate model performances\n",
    "\n",
    "In order to finetune the model, some features are added to enable the model to update its parameters via (triplet) loss function:  \n",
    "1. Negative Responses and their respective context placeholders.\n",
    "2. Optimizer. Importantly, `var_finetune` allows selective update of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model initiated!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import tf_sentencepiece\n",
    "from src.metric_learning import triplet_loss, contrastive_loss\n",
    "from tensorflow.train import Saver\n",
    "from src.utils import split_txt, read_txt, clean_txt, read_kb_csv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Set up graph.\n",
    "tf.reset_default_graph() # finetune\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "\n",
    "    # load USE as a 'module', which is tf_hub's interfacable transfer learning component\n",
    "    embed = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/1\", trainable=True)\n",
    "    \n",
    "    # put variable placeholders\n",
    "    question = tf.placeholder(dtype=tf.string, shape=[None])  \n",
    "    response = tf.placeholder(dtype=tf.string, shape=[None])  \n",
    "    response_context = tf.placeholder(dtype=tf.string, shape=[None])\n",
    "\n",
    "\n",
    "    # QnA embeddings\n",
    "    question_embeddings = embed(dict(input=question),\n",
    "                                signature=\"question_encoder\", as_dict=True)\n",
    "\n",
    "    response_embeddings = embed(dict(input=response,\n",
    "                                    context=response_context),\n",
    "                                signature=\"response_encoder\", as_dict=True)\n",
    "    \n",
    "    # negative response placeholder and embeddings\n",
    "    # triplet loss requires a negative example\n",
    "    neg_response = tf.placeholder(dtype=tf.string, shape=[None])  \n",
    "    neg_response_context = tf.placeholder(dtype=tf.string, shape=[None]) \n",
    "    neg_response_embeddings = embed(dict(input=neg_response,\n",
    "                                        context=neg_response_context),\n",
    "                                    signature=\"response_encoder\", as_dict=True)\n",
    "\n",
    "    # However, we may instead choose to have a contrastive loss\n",
    "    # this requires a label rather than a negative example\n",
    "    label = tf.placeholder(tf.int32, [None], name='label')\n",
    "    \n",
    "    # either triplet or contrastive loss\n",
    "    cost = triplet_loss(question_embeddings['outputs'], response_embeddings['outputs'], neg_response_embeddings['outputs'], margin=0.3)\n",
    "\n",
    "    # init operation\n",
    "    init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    opt = tf.train.GradientDescentOptimizer(learning_rate=0.6)\n",
    "    \n",
    "    # get the weights we want to finetune.\n",
    "    # the embed object has parameters and the parameters are named\n",
    "    # we list the variables that we want to tune in v\n",
    "    # and put them into var_list so tensorflow will only change \n",
    "    # these specific params\n",
    "    v=['module/QA/Final/Response_tuning/ResidualHidden_1/AdjustDepth/projection/kernel']\n",
    "    var_finetune=[x for x in embed.variables for vv in v if vv in x.name] \n",
    "    opt_op = opt.minimize(cost, var_list=var_finetune)\n",
    "\n",
    "# Initialize session\n",
    "session = tf.Session(graph=g, config=tf.ConfigProto(log_device_placement=False))\n",
    "session.run(init_op)\n",
    "print('model initiated!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### A simple finetuning can be carried out in loops carrying out 3 steps each loop:\n",
    "1. Record loss\n",
    "2. Run optimizer\n",
    "3. Record accuracy\n",
    "\n",
    "A simple train-test set is built for the finetuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomized_idx = np.random.permutation(np.arange(len(answers_)))\n",
    "train_idx = randomized_idx[:200].tolist()\n",
    "test_idx = randomized_idx[200:].tolist()\n",
    "\n",
    "train_ans, train_ques = answers_[train_idx].tolist(), ques_[train_idx].tolist()\n",
    "test_ans, test_ques = answers_[test_idx].tolist(), ques_[test_idx].tolist()\n",
    "\n",
    "train_neg_ans = answers_[np.random.permutation(train_idx)].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Record loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.026533306"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_loss = session.run(cost, \n",
    "                           feed_dict={\n",
    "                                    question:train_ques,\n",
    "                                    response:train_ans,\n",
    "                                    response_context:train_ans,\n",
    "                                    neg_response:train_neg_ans,\n",
    "                                    neg_response_context:train_neg_ans,\n",
    "                                    #label:label_\n",
    "                                    })\n",
    "current_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "##### 2. Update params (optimize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.run(opt_op, \n",
    "           feed_dict={\n",
    "                    question:train_ques,\n",
    "                    response:train_ans,\n",
    "                    response_context:train_ans,\n",
    "                    neg_response:train_neg_ans,\n",
    "                    neg_response_context:train_neg_ans,\n",
    "                    #label:label_\n",
    "                    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "###### 3. Record Accuracy\n",
    "Accuracy for the QA model works by:\n",
    "1. Rank solutions to each question \n",
    "2. Add 1 if correct answer is in top 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranker(session, question_list, answer_list):\n",
    "    \"\"\"for model evaluation on InsuranceQA datset\"\"\"\n",
    "    predictions=[]\n",
    "    for ii, question_str in enumerate(question_list):\n",
    "        ques_vectors = session.run(response_embeddings, \n",
    "                                 feed_dict={response:[question_str],\n",
    "                                            response_context:[question_str]\n",
    "                                           })['outputs']\n",
    "        \n",
    "        doc_vectors = session.run(response_embeddings, \n",
    "                                 feed_dict={response:answer_list,\n",
    "                                            response_context:answer_list\n",
    "                                           })['outputs']\n",
    "        \n",
    "        cossim = cosine_similarity(doc_vectors, ques_vectors.reshape(1, -1))\n",
    "        sortargs=np.flip(cossim.argsort(axis=0))\n",
    "        returnedans = [answer_list[jj[0]] for jj in sortargs]\n",
    "        predictions.append(returnedans)\n",
    "        \n",
    "    return predictions\n",
    "\n",
    "def scorer(predictions, gts, k=3):\n",
    "    \"\"\"For model evaluation on InsuranceQA datset. Returns score@k.\"\"\"\n",
    "    score=0\n",
    "    total=0\n",
    "    for gt, prediction in zip(gts, predictions):\n",
    "        if bool(set([gt]) & set(prediction[:k])):\n",
    "            score+=1\n",
    "        total+=1\n",
    "    return score/total\n",
    "\n",
    "# may take a while\n",
    "predictionss = ranker(session, test_ques, test_ans)\n",
    "\n",
    "# check accuracy score\n",
    "acc_score = scorer( predictionss, test_ans , k=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Loop through all 3 steps in each loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Loss : 0.02144646644592285\n",
      "Score @1 : 0.5806451612903226\n",
      "Score @2 : 0.7580645161290323\n",
      "Score @3 : 0.8387096774193549\n",
      "Score @4 : 0.8709677419354839\n",
      "\n",
      "Epoch 1\n",
      "Loss : 0.016312837600708008\n",
      "Score @1 : 0.5806451612903226\n",
      "Score @2 : 0.7580645161290323\n",
      "Score @3 : 0.8387096774193549\n",
      "Score @4 : 0.8709677419354839\n",
      "\n",
      "Epoch 2\n",
      "Loss : 0.011141598224639893\n",
      "Score @1 : 0.5806451612903226\n",
      "Score @2 : 0.7580645161290323\n",
      "Score @3 : 0.8387096774193549\n",
      "Score @4 : 0.8709677419354839\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "        \n",
    "    # keep track of current loss\n",
    "    current_loss = session.run(cost, \n",
    "                               feed_dict={  question:train_ques,\n",
    "                                            response:train_ans,\n",
    "                                            response_context:train_ans,\n",
    "                                            neg_response:train_neg_ans,\n",
    "                                            neg_response_context:train_neg_ans,\n",
    "                                            #label:label_\n",
    "                                            })\n",
    "    print(\"Epoch {}\".format(i))\n",
    "    print(\"Loss : {}\".format(current_loss))\n",
    "\n",
    "    \n",
    "    # update params\n",
    "    session.run(opt_op, \n",
    "                feed_dict={ question:train_ques,\n",
    "                            response:train_ans,\n",
    "                            response_context:train_ans,\n",
    "                            neg_response:train_neg_ans,\n",
    "                            neg_response_context:train_neg_ans,\n",
    "                            #label:label_\n",
    "                            })\n",
    "\n",
    "    # create predictions to test accuracy\n",
    "    # may take a while\n",
    "    predictionss = ranker(session, test_ques, test_ans)\n",
    "    # check accuracy score\n",
    "    for k_ in [1,2,3,4]:\n",
    "        acc_score = scorer( predictionss, test_ans , k=k_)\n",
    "        print(\"Score @{} : {}\".format(k_, acc_score))\n",
    "    \n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to test out document selection\n",
    "Using Squad DB stored in SQL server, First test out using gensim's LSI, then bm25 to see document retrieval rate and think of ideas to improve.  \n",
    "## Experiment design\n",
    "### 1. Dataset\n",
    "The dataset used will be Squad train set, as stored in a hosted SQL server. All the clauses from each wikipedia article are re-aggregated into the same document. There are a total of 442 documents.\n",
    "Queries are also from the same Squad train set. Query labels are the index of the document to which the question applies. There are 87599 queries.\n",
    "### 2. Test method\n",
    "Each method, along with any necessary hyperparameters will be tested on the dataset. Both accuracy and speed should be logged. In terms of metrics, Possibly accuracy @1 to @5. (What about percentile of correct answer!)\n",
    "Will try to utilize multiple cores using multiprocessing module.\n",
    "### 3. Test subjects\n",
    "1. BM25\n",
    "2. LSI with 2 topics\n",
    "3. LSI with 20 topics\n",
    "4. LSI with 200 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from gensim import corpora, models, similarities\n",
    "from collections import defaultdict\n",
    "from src.dataloader import document_retrieval\n",
    "from gensim.summarization import bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_kb, df_query = document_retrieval('../db_cnxn_str.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSIModel:\n",
    "    def __init__(self, df, num_topics=2):\n",
    "        self.ind2id = {ind:id for ind, id in zip(df.index,df['id'])}\n",
    "        stoplist = set('for a of the and to in'.split())\n",
    "        texts = [\n",
    "            [word for word in document.lower().split() if word not in stoplist]\n",
    "            for document in df['raw_txt']\n",
    "        ]\n",
    "        # remove words that appear only once\n",
    "        frequency = defaultdict(int)\n",
    "        for text in texts:\n",
    "            for token in text:\n",
    "                frequency[token] += 1\n",
    "\n",
    "        texts = [\n",
    "            [token for token in text if frequency[token] > 1]\n",
    "            for text in texts\n",
    "        ]\n",
    "\n",
    "        self.dictionary = corpora.Dictionary(texts)\n",
    "        self.corpus = [self.dictionary.doc2bow(text) for text in texts]\n",
    "        self.lsi = models.LsiModel(self.corpus, id2word=self.dictionary, num_topics=num_topics)  \n",
    "        self.index = similarities.MatrixSimilarity(self.lsi[self.corpus])  # transform corpus to LSI space and index it\n",
    "        \n",
    "    def predict(self, query):\n",
    "        vec_bow = self.dictionary.doc2bow(query.lower().split())\n",
    "        vec_lsi = self.lsi[vec_bow]  # convert the query to LSI space\n",
    "        sims = self.index[vec_lsi]  # perform a similarity query against the corpus\n",
    "        sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "        return sims       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9984453 The EPS user interface management system\n",
      "0.998093 Human machine interface for lab abc computer applications\n",
      "0.9865886 System and human system engineering testing of EPS\n",
      "0.93748635 A survey of user opinion of computer system response time\n",
      "0.90755945 Relation of user perceived response time to error measurement\n",
      "0.050041765 Graph minors A survey\n",
      "-0.09879464 Graph minors IV Widths of trees and well quasi ordering\n",
      "-0.10639259 The intersection graph of paths in trees\n",
      "-0.12416792 The generation of random binary unordered trees\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "test_df = pd.DataFrame({'raw_txt':[\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "], 'id':[i for i in range(9)]})\n",
    "\n",
    "test_lsimodel = LSIModel(test_df, num_topics=2)\n",
    "# test_lsi, test_ind2id, test_dict, test_corpus = prep_model(test_df)\n",
    "test_sims = test_lsimodel.predict('Human computer interaction')\n",
    "# print(test_sims)\n",
    "for ii, (i, s) in enumerate(test_sims):\n",
    "    print(s, test_df['raw_txt'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyBM25(bm25.BM25):\n",
    "    def __init__(self, df):\n",
    "        super().__init__(df['raw_txt'])\n",
    "        self.ind2id = {ind:id for ind, id in zip(df.index,df['id'])}\n",
    "    def predict(self, query):\n",
    "        scores = self.get_scores(query)\n",
    "        scores = sorted(enumerate(scores), key=lambda item: item[1])\n",
    "        return scores\n",
    "#         scores.index(max(scores))\n",
    "#         display(df_kb[df_kb['id']==ind2id[scores.index(max(scores))]][['id', 'kb_name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(ans_ranks, counter):\n",
    "    for acc in range(1,6):\n",
    "        correct = sum(map(lambda x : x<acc, ans_ranks))\n",
    "        print('accuracy @ {}: {:.4f}'.format(acc, correct/counter))\n",
    "    print('ranking percentile {:.2f}'.format( 1-sum(ans_ranks)/len(ans_ranks)/448))\n",
    "    \n",
    "def scoring_function(df_query, modelobj):\n",
    "    '''Return accuacy at 1 through 5 docs, and average rank of correct ans.'''\n",
    "    counter=0\n",
    "    ans_ranks=[]\n",
    "    for _, row in df_query.sample(frac=1, random_state=42).iterrows():\n",
    "        ranking = modelobj.predict(row['query_string'])\n",
    "        ranked_doc_ids = [modelobj.ind2id[k] for k, v in ranking]\n",
    "        ans_ranks.append(ranked_doc_ids.index(row['doc_id']))\n",
    "        counter+=1\n",
    "        if counter%20000==0:\n",
    "            print_score(ans_ranks, counter)\n",
    "    print('final score: ') \n",
    "    print_score(ans_ranks, counter)\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "models=[LSIModel(df_kb, num_topics=6000),\n",
    "        LSIModel(df_kb, num_topics=4000),\n",
    "        LSIModel(df_kb, num_topics=2000),\n",
    "        LSIModel(df_kb, num_topics=200),\n",
    " LSIModel(df_kb, num_topics=20),\n",
    " LSIModel(df_kb, num_topics=2),\n",
    "MyBM25(df_kb)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy @ 1: 0.2898\n",
      "accuracy @ 2: 0.3513\n",
      "accuracy @ 3: 0.3842\n",
      "accuracy @ 4: 0.4088\n",
      "accuracy @ 5: 0.4273\n",
      "ranking percentile 0.88\n",
      "accuracy @ 1: 0.2907\n",
      "accuracy @ 2: 0.3529\n",
      "accuracy @ 3: 0.3869\n",
      "accuracy @ 4: 0.4101\n",
      "accuracy @ 5: 0.4273\n",
      "ranking percentile 0.88\n",
      "accuracy @ 1: 0.2909\n",
      "accuracy @ 2: 0.3529\n",
      "accuracy @ 3: 0.3867\n",
      "accuracy @ 4: 0.4100\n",
      "accuracy @ 5: 0.4273\n",
      "ranking percentile 0.88\n",
      "accuracy @ 1: 0.2913\n",
      "accuracy @ 2: 0.3538\n",
      "accuracy @ 3: 0.3873\n",
      "accuracy @ 4: 0.4106\n",
      "accuracy @ 5: 0.4279\n",
      "ranking percentile 0.88\n",
      "final score: \n",
      "accuracy @ 1: 0.2914\n",
      "accuracy @ 2: 0.3540\n",
      "accuracy @ 3: 0.3875\n",
      "accuracy @ 4: 0.4107\n",
      "accuracy @ 5: 0.4280\n",
      "ranking percentile 0.88\n",
      "accuracy @ 1: 0.2898\n",
      "accuracy @ 2: 0.3513\n",
      "accuracy @ 3: 0.3842\n",
      "accuracy @ 4: 0.4088\n",
      "accuracy @ 5: 0.4273\n",
      "ranking percentile 0.88\n",
      "accuracy @ 1: 0.2907\n",
      "accuracy @ 2: 0.3529\n",
      "accuracy @ 3: 0.3869\n",
      "accuracy @ 4: 0.4101\n",
      "accuracy @ 5: 0.4273\n",
      "ranking percentile 0.88\n",
      "accuracy @ 1: 0.2909\n",
      "accuracy @ 2: 0.3529\n",
      "accuracy @ 3: 0.3867\n",
      "accuracy @ 4: 0.4100\n",
      "accuracy @ 5: 0.4273\n",
      "ranking percentile 0.88\n",
      "accuracy @ 1: 0.2913\n",
      "accuracy @ 2: 0.3538\n",
      "accuracy @ 3: 0.3873\n",
      "accuracy @ 4: 0.4106\n",
      "accuracy @ 5: 0.4279\n",
      "ranking percentile 0.88\n",
      "final score: \n",
      "accuracy @ 1: 0.2914\n",
      "accuracy @ 2: 0.3540\n",
      "accuracy @ 3: 0.3875\n",
      "accuracy @ 4: 0.4107\n",
      "accuracy @ 5: 0.4280\n",
      "ranking percentile 0.88\n",
      "accuracy @ 1: 0.2898\n",
      "accuracy @ 2: 0.3513\n",
      "accuracy @ 3: 0.3842\n",
      "accuracy @ 4: 0.4088\n",
      "accuracy @ 5: 0.4273\n",
      "ranking percentile 0.88\n",
      "accuracy @ 1: 0.2907\n",
      "accuracy @ 2: 0.3529\n",
      "accuracy @ 3: 0.3869\n",
      "accuracy @ 4: 0.4101\n",
      "accuracy @ 5: 0.4273\n",
      "ranking percentile 0.88\n",
      "accuracy @ 1: 0.2909\n",
      "accuracy @ 2: 0.3529\n",
      "accuracy @ 3: 0.3867\n",
      "accuracy @ 4: 0.4100\n",
      "accuracy @ 5: 0.4273\n",
      "ranking percentile 0.88\n",
      "accuracy @ 1: 0.2913\n",
      "accuracy @ 2: 0.3538\n",
      "accuracy @ 3: 0.3873\n",
      "accuracy @ 4: 0.4106\n",
      "accuracy @ 5: 0.4279\n",
      "ranking percentile 0.88\n",
      "final score: \n",
      "accuracy @ 1: 0.2914\n",
      "accuracy @ 2: 0.3540\n",
      "accuracy @ 3: 0.3875\n",
      "accuracy @ 4: 0.4107\n",
      "accuracy @ 5: 0.4280\n",
      "ranking percentile 0.88\n",
      "accuracy @ 1: 0.2083\n",
      "accuracy @ 2: 0.2587\n",
      "accuracy @ 3: 0.2895\n",
      "accuracy @ 4: 0.3116\n",
      "accuracy @ 5: 0.3301\n",
      "ranking percentile 0.84\n",
      "accuracy @ 1: 0.2121\n",
      "accuracy @ 2: 0.2625\n",
      "accuracy @ 3: 0.2925\n",
      "accuracy @ 4: 0.3143\n",
      "accuracy @ 5: 0.3326\n",
      "ranking percentile 0.84\n",
      "accuracy @ 1: 0.2127\n",
      "accuracy @ 2: 0.2628\n",
      "accuracy @ 3: 0.2928\n",
      "accuracy @ 4: 0.3142\n",
      "accuracy @ 5: 0.3324\n",
      "ranking percentile 0.84\n",
      "accuracy @ 1: 0.2136\n",
      "accuracy @ 2: 0.2639\n",
      "accuracy @ 3: 0.2938\n",
      "accuracy @ 4: 0.3153\n",
      "accuracy @ 5: 0.3334\n",
      "ranking percentile 0.83\n",
      "final score: \n",
      "accuracy @ 1: 0.2139\n",
      "accuracy @ 2: 0.2641\n",
      "accuracy @ 3: 0.2938\n",
      "accuracy @ 4: 0.3153\n",
      "accuracy @ 5: 0.3332\n",
      "ranking percentile 0.83\n",
      "accuracy @ 1: 0.0491\n",
      "accuracy @ 2: 0.0772\n",
      "accuracy @ 3: 0.0993\n",
      "accuracy @ 4: 0.1141\n",
      "accuracy @ 5: 0.1273\n",
      "ranking percentile 0.74\n",
      "accuracy @ 1: 0.0511\n",
      "accuracy @ 2: 0.0786\n",
      "accuracy @ 3: 0.0993\n",
      "accuracy @ 4: 0.1148\n",
      "accuracy @ 5: 0.1274\n",
      "ranking percentile 0.74\n",
      "accuracy @ 1: 0.0517\n",
      "accuracy @ 2: 0.0795\n",
      "accuracy @ 3: 0.1002\n",
      "accuracy @ 4: 0.1157\n",
      "accuracy @ 5: 0.1281\n",
      "ranking percentile 0.74\n",
      "accuracy @ 1: 0.0517\n",
      "accuracy @ 2: 0.0794\n",
      "accuracy @ 3: 0.0995\n",
      "accuracy @ 4: 0.1155\n",
      "accuracy @ 5: 0.1280\n",
      "ranking percentile 0.74\n",
      "final score: \n",
      "accuracy @ 1: 0.0518\n",
      "accuracy @ 2: 0.0793\n",
      "accuracy @ 3: 0.0995\n",
      "accuracy @ 4: 0.1155\n",
      "accuracy @ 5: 0.1283\n",
      "ranking percentile 0.74\n",
      "accuracy @ 1: 0.0044\n",
      "accuracy @ 2: 0.0077\n",
      "accuracy @ 3: 0.0126\n",
      "accuracy @ 4: 0.0163\n",
      "accuracy @ 5: 0.0218\n",
      "ranking percentile 0.64\n",
      "accuracy @ 1: 0.0044\n",
      "accuracy @ 2: 0.0079\n",
      "accuracy @ 3: 0.0129\n",
      "accuracy @ 4: 0.0167\n",
      "accuracy @ 5: 0.0221\n",
      "ranking percentile 0.64\n",
      "accuracy @ 1: 0.0045\n",
      "accuracy @ 2: 0.0081\n",
      "accuracy @ 3: 0.0132\n",
      "accuracy @ 4: 0.0170\n",
      "accuracy @ 5: 0.0220\n",
      "ranking percentile 0.64\n",
      "accuracy @ 1: 0.0044\n",
      "accuracy @ 2: 0.0080\n",
      "accuracy @ 3: 0.0134\n",
      "accuracy @ 4: 0.0174\n",
      "accuracy @ 5: 0.0227\n",
      "ranking percentile 0.64\n",
      "final score: \n",
      "accuracy @ 1: 0.0044\n",
      "accuracy @ 2: 0.0081\n",
      "accuracy @ 3: 0.0134\n",
      "accuracy @ 4: 0.0176\n",
      "accuracy @ 5: 0.0230\n",
      "ranking percentile 0.64\n",
      "accuracy @ 1: 0.0238\n",
      "accuracy @ 2: 0.0352\n",
      "accuracy @ 3: 0.0428\n",
      "accuracy @ 4: 0.0496\n",
      "accuracy @ 5: 0.0543\n",
      "ranking percentile 0.60\n",
      "accuracy @ 1: 0.0234\n",
      "accuracy @ 2: 0.0351\n",
      "accuracy @ 3: 0.0428\n",
      "accuracy @ 4: 0.0491\n",
      "accuracy @ 5: 0.0542\n",
      "ranking percentile 0.60\n",
      "accuracy @ 1: 0.0243\n",
      "accuracy @ 2: 0.0358\n",
      "accuracy @ 3: 0.0432\n",
      "accuracy @ 4: 0.0492\n",
      "accuracy @ 5: 0.0545\n",
      "ranking percentile 0.60\n",
      "accuracy @ 1: 0.0242\n",
      "accuracy @ 2: 0.0358\n",
      "accuracy @ 3: 0.0428\n",
      "accuracy @ 4: 0.0491\n",
      "accuracy @ 5: 0.0546\n",
      "ranking percentile 0.60\n",
      "final score: \n",
      "accuracy @ 1: 0.0240\n",
      "accuracy @ 2: 0.0357\n",
      "accuracy @ 3: 0.0426\n",
      "accuracy @ 4: 0.0490\n",
      "accuracy @ 5: 0.0545\n",
      "ranking percentile 0.60\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    scoring_function(df_query, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hotdoc] *",
   "language": "python",
   "name": "conda-env-hotdoc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
